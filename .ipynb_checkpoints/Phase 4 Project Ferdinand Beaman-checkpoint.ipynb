{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we lemmatize our text data, or leave the words as is?\n",
    "\n",
    "Mention the reason why it's a binary and not a ternary classifier.\n",
    "\n",
    "Mention\n",
    "\n",
    "What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword languages? imbalanced dataset with smote?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:45:09.257609Z",
     "start_time": "2024-02-26T07:45:08.974600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ferdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.549996Z",
     "start_time": "2024-02-24T06:34:10.513067Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"BrandsAndProductEmotions.csv\", encoding='unicode_escape')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names following the @ symbol are probably useless, but since they do include terms that might be seen elsewhere (e.g. sxsw) I may leave them in. Any of the ones that only show up once or twice will be taken out by the tokenization process regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those column names are a mouthful. And the last row is definitely not made for human eyes so I'll just cut it right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.554711Z",
     "start_time": "2024-02-24T06:34:10.551333Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [\"text\", \"product\", \"emotion\"]\n",
    "df.drop(df.index[9092], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if there's anything interesting from a macro level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.562562Z",
     "start_time": "2024-02-24T06:34:10.555905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "No emotion toward brand or product    5388\n",
      "Positive emotion                      2978\n",
      "Negative emotion                       570\n",
      "I can't tell                           156\n",
      "Name: emotion, dtype: int64\n",
      "\n",
      "Products\n",
      "iPad                               946\n",
      "Apple                              661\n",
      "iPad or iPhone App                 470\n",
      "Google                             430\n",
      "iPhone                             297\n",
      "Other Google product or service    293\n",
      "Android App                         81\n",
      "Android                             78\n",
      "Other Apple product or service      35\n",
      "Name: product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Value counts for the two categorical columns\n",
    "print(\"Emotion\")\n",
    "print(df.emotion.value_counts())\n",
    "print(\"\\nProducts\")\n",
    "print(df[\"product\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T06:41:19.164476Z",
     "start_time": "2024-02-14T06:41:19.157031Z"
    }
   },
   "source": [
    "570 is only a little over 6% of the whole dataset. I may need to address this class imbalance later when building my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.573969Z",
     "start_time": "2024-02-24T06:34:10.563523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9092, 3)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9091\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     9091 non-null   object\n",
      " 1   product  3291 non-null   object\n",
      " 2   emotion  9092 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 284.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(\"\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, one of the text entries is null, and nearly 2/3s of the rest of the entries are missing a product label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.578976Z",
     "start_time": "2024-02-24T06:34:10.574866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text product                             emotion\n",
       "6  NaN     NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before I forget, let me look at that null entry\n",
    "df[df[\"text\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictably, this was useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.582549Z",
     "start_time": "2024-02-24T06:34:10.579787Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[6], inplace = True)\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at one of those tweets that's missing a product label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.586634Z",
     "start_time": "2024-02-24T06:34:10.584424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[9090][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, this is just a generic tweet. How did this end up here? It also was labeled as lacking an emotional quality. Is that what all of the null entries in the product column are like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.592045Z",
     "start_time": "2024-02-24T06:34:10.588068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5296\n",
       "Positive emotion                       306\n",
       "I can't tell                           147\n",
       "Negative emotion                        51\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"product\"].isnull()][\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, there are emotional statements in here. This seems to be *somewhat* similar to the rest of the dataset except for a big dropoff for positive tweets.\n",
    "\n",
    "Anyway, I have two tasks to start with: seeing if the NaN entries in the \"emotion\" column  are worth keeping, and investigating the \"I can't tell\"s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.601186Z",
     "start_time": "2024-02-24T06:34:10.592834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT Hiring marketers, designers, creatives, social media pros... Come see #Aquent booth 1415 #SXSW trade show. Might win iPad 2\n",
      "\n",
      "\n",
      "RT @mention Full #SXSW #touchingstories presentation: {link}\n",
      "\n",
      "\n",
      "RT @mention #SXSW Interactive Award: Music Category goes to &quot;Wilderness Downtown&quot;. Congrats shared with @mention @mention @mention #winning\n",
      "\n",
      "\n",
      "One guy stakes out the Austin Apple popup shop at #SXSW for his #iPad 2 {link} #SXSWi\n",
      "\n",
      "\n",
      "RT @mention RT @mention Google set to launch new social network #Circles today at #sxsw\n",
      "\n",
      "\n",
      "hey @mention heard you're at #sxsw. Come by to the @mention grille and make your comic into a iPhone case? What do you say? :]\n",
      "\n",
      "\n",
      "Too bad I don't have a _ button!\n",
      "RT @mention I know its #SXSW time when I have an abnormal amount of app updates on my iPhone.\n",
      "\n",
      "\n",
      "RT @mention &quot;my kids will not grow up thinking the New York Times and Google are in separate industries&quot; @mention #bvj #SXSW\n",
      "\n",
      "\n",
      "packing for #sxsw = iPad, iPhone, BlackBerry, laptop, and video camera. Need a stylish belt-clip-gadget-holster. Or is that an oxymoron? #in\n",
      "\n",
      "\n",
      "The line outside the #SXSW Apple pop-up store. #wasteoftimeatsxsw {link}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_prod_s = df[df[\"product\"].isnull() == True].sample(n=10, random_state =2)\n",
    "for i in range(10):\n",
    "    print(null_prod_s[\"text\"].iloc[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, I get it now: the collection of tweets is just the result of scraping and collecting any tweets that have tech-company keywords in them. Since this sounds like a realistic version of what would happen in the real world, I think I should leave these in and hope my model can accurately toss them aside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"I can't tell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.606701Z",
     "start_time": "2024-02-24T06:34:10.601930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprando mi iPad 2 en el #SXSW (@mention Apple Store, SXSW w/ 62 others) {link}\n",
      "\n",
      "\n",
      "The queue at the Apple Store in Austin is FOUR blocks long. Crazy stuff! #sxsw\n",
      "\n",
      "\n",
      "RT @mention Demo of Google Hotpot at #bettersearch panel: still pull search, but personalized. Not yet serendipitous? #SXSW\n",
      "\n",
      "\n",
      "Why Barry Diller thinks iPad only content is nuts @mention #SXSW {link}\n",
      "\n",
      "\n",
      "Like @mention I've now seen most of Austin in Google Streetview checking out apartments for #sxsw. Austin is not easy on the click.\n",
      "\n",
      "\n",
      "Anyone know status of iPad 2s in Austin pop-up store? Sold out? Getting more? #ipad2 #sxsw\n",
      "\n",
      "\n",
      "Reports of @mention introducing a new social media platform at #SXSW were premature, but hopefully not overly optimistic {link}\n",
      "\n",
      "\n",
      "DANG RT @mention Confirmed! Apple store 2 week popup in Austin for #SXSW {link} (via @mention who gave us no credit! )\n",
      "\n",
      "\n",
      "At the Team Android party. Can't find it on Gowalla or Foursquare, so um, there you go. #sxsw\n",
      "\n",
      "\n",
      "Line for Source Code is even longer than for iPad 2. Take that, Apple. #sxsw\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ict_emo_s = df[df[\"emotion\"] == \"I can't tell\"].sample(n=10, random_state =2)\n",
    "for i in range(10):\n",
    "    print(ict_emo_s[\"text\"].iloc[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh hello Spanish. This possibility didn't cross my mind at all. But, if they're commonplace, then they will end up having enough words make it through the commonness filter later on. If this is a one-time occurrence, then all of the words will be filtered out.\n",
    "\n",
    "In any case, I can see why these messages were left out. Since there aren't that many of them, we can safely remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.610664Z",
     "start_time": "2024-02-24T06:34:10.607511Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[\"emotion\"].loc[df[\"emotion\"]==\"I can't tell\"].index)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "#https://stackoverflow.com/questions/53182464/\n",
    "#pandas-delete-a-row-in-a-dataframe-based-on-a-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.614778Z",
     "start_time": "2024-02-24T06:34:10.611455Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5387\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I moved forward from here, I would likely not have enough negative tweets to create a proper ternary classifier. So, instead, I'll group the negative tweets in with the neutral tweets to create something that's simply \"not positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.620660Z",
     "start_time": "2024-02-24T06:34:10.615827Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['emotion'] == \"No emotion toward brand or product\", \"emotion\"\n",
    "      ] = \"Not positive\"\n",
    "\n",
    "df.loc[df['emotion'] == \"Negative emotion\", \"emotion\"] = \"Not positive\"\n",
    "\n",
    "df.loc[df['emotion'] == \"Positive emotion\", \"emotion\"] = \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.624666Z",
     "start_time": "2024-02-24T06:34:10.621414Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not positive    5957\n",
       "Positive        2978\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know how much it will help, but I figured I may as well create a column which identifies the company each tweet refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:10.628469Z",
     "start_time": "2024-02-24T06:34:10.625384Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               942\n",
       "Apple                              659\n",
       "iPad or iPhone App                 470\n",
       "Google                             429\n",
       "iPhone                             296\n",
       "Other Google product or service    292\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"product\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:11.546814Z",
     "start_time": "2024-02-24T06:34:10.629304Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Not positive</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Not positive</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "        emotion company  \n",
       "0  Not positive   Apple  \n",
       "1      Positive   Apple  \n",
       "2      Positive   Apple  \n",
       "3  Not positive   Apple  \n",
       "4      Positive  Google  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"company\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    if \"Google\" in str(df[\"product\"].iloc[i]):\n",
    "        df[\"company\"].iloc[i] = \"Google\"\n",
    "    elif \"Android\" in str(df[\"product\"].iloc[i]):\n",
    "        df[\"company\"].iloc[i] = \"Android\"\n",
    "    else:\n",
    "        df[\"company\"].iloc[i] = \"Apple\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:11.550732Z",
     "start_time": "2024-02-24T06:34:11.547588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple      8055\n",
       "Google      721\n",
       "Android     159\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"company\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly didn't expect the dataset to be so imbalanced, but I guess these tweets were made around some Apple-related event?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Flatiron Approved preprocessing steps (lower case, remove punctuation, lemmatize), I'll also expand the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:11.552953Z",
     "start_time": "2024-02-24T06:34:11.551578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:12.559450Z",
     "start_time": "2024-02-24T06:34:11.553699Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    exp_text = []\n",
    "    for word in df[\"text\"].iloc[i].split():\n",
    "        exp_text.append(contractions.fix(word))\n",
    "    exp_text = \" \".join(exp_text)\n",
    "    df[\"processed_text\"].iloc[i] = exp_text\n",
    "    \n",
    "df.drop(\"text\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:12.562600Z",
     "start_time": "2024-02-24T06:34:12.560329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you will likely appreciate for its design. Also, they are giving free Ts at #SXSW'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.processed_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:13.260418Z",
     "start_time": "2024-02-24T06:34:12.563375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "for i in range(len(df)):\n",
    "    df.processed_text.iloc[i] = \" \".join(tokenizer.tokenize(df.processed_text.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:13.270699Z",
     "start_time": "2024-02-24T06:34:13.264324Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df[\"processed_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:13.279311Z",
     "start_time": "2024-02-24T06:34:13.274294Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>emotion</th>\n",
       "      <th>company</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>Not positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>wesley83 have 3g iphone after hrs tweeting at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>jessedee know about fludapp awesome ipad iphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>swonderlin can not wait for ipad also they sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Not positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>sxsw hope this year festival is not as crashy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>sxtxstate great stuff on fri sxsw marissa maye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              product       emotion company  \\\n",
       "0              iPhone  Not positive   Apple   \n",
       "1  iPad or iPhone App      Positive   Apple   \n",
       "2                iPad      Positive   Apple   \n",
       "3  iPad or iPhone App  Not positive   Apple   \n",
       "4              Google      Positive  Google   \n",
       "\n",
       "                                      processed_text  \n",
       "0  wesley83 have 3g iphone after hrs tweeting at ...  \n",
       "1  jessedee know about fludapp awesome ipad iphon...  \n",
       "2  swonderlin can not wait for ipad also they sho...  \n",
       "3  sxsw hope this year festival is not as crashy ...  \n",
       "4  sxtxstate great stuff on fri sxsw marissa maye...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've known how to hunt down duplicates since nearly the start of my program. But the strings that I'm looking for aren't exact replicas of one another. Many of them have just \"rt\" or \"link\" somewhere. Or a typo.\n",
    "\n",
    "Luckily, after quite a lot of searching, I was able to find a library that handles near-duplicates. It's called \"thefuzz\" (previously called fuzzywuzzy) and it shows you how much you would have to edit one message to create another through its ratio method. With this method, we can compare the similarity of two different messages in order to find a cutoff for where the messages are likely too similar to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.744238Z",
     "start_time": "2024-02-24T06:34:13.280340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thefuzz in /Users/ferdi/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (0.22.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /Users/ferdi/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from thefuzz) (3.6.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/ferdi/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (3.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.749224Z",
     "start_time": "2024-02-24T06:34:15.746517Z"
    }
   },
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, with that out of the way, the following two messages seem like a typical example of an original tweet and a retweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.760039Z",
     "start_time": "2024-02-24T06:34:15.750879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung sony follow apple hp lead mention link austin atx sxsw\n",
      "samsung sony follow apple hp lead mention link austin atx sxsw via mention rg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"processed_text\"][130])\n",
    "print(df[\"processed_text\"][131])\n",
    "# This should tell me the percent-similarity of two strings\n",
    "fuzz.ratio(df[\"processed_text\"][130], df[\"processed_text\"][131])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T06:23:19.751714Z",
     "start_time": "2024-02-19T06:23:19.745902Z"
    }
   },
   "source": [
    "If this is an example of 89% similarity, I assume I can expand my scope a bit but I'm not sure to where. First guess? I'll look down through those above 75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.835855Z",
     "start_time": "2024-02-24T06:34:15.761517Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup_above_75 = []\n",
    "\n",
    "for x in range(len(df)-2): # Minus two\n",
    "    tweet1 = df[\"processed_text\"][x]\n",
    "    tweet2  = df[\"processed_text\"][x+1]\n",
    "    if fuzz.ratio(tweet1, tweet2) >75:\n",
    "        dup_above_75.append(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.838857Z",
     "start_time": "2024-02-24T06:34:15.836849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "print(len(dup_above_75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "778 duplicates, just for adjacent tweets. And how many of those duplicate tweets ended up with a different emotional categorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:15.848049Z",
     "start_time": "2024-02-24T06:34:15.839912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2326478149100257"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_above_75 = 0\n",
    "for tweet in dup_above_75:\n",
    "    if df[\"emotion\"][tweet] == df[\"emotion\"][tweet-1]:\n",
    "        pass\n",
    "    else:\n",
    "        dif_above_75 += 1\n",
    "\n",
    "dif_above_75/len(dup_above_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, I didn't expect a quarter of the messages in this range to be so different. I suppose 75 was far too low. What's the baseline, and when does that number dip below 10%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:16.324689Z",
     "start_time": "2024-02-24T06:34:15.849174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%ers match 0.5858054405015113 of the time\n",
      "5%ers match 0.5858054405015113 of the time\n",
      "10%ers match 0.5858054405015113 of the time\n",
      "15%ers match 0.5858054405015113 of the time\n"
     ]
    }
   ],
   "source": [
    "#Low similarity, to establish a baseline\n",
    "for percent in range(0,16,5):\n",
    "    diff_emo = 0\n",
    "    dupes = 0\n",
    "    for x in range(len(df)-2):\n",
    "        tweet1 = df[\"processed_text\"][x]\n",
    "        tweet2  = df[\"processed_text\"][x+1]\n",
    "        emo1 = df[\"emotion\"][x]\n",
    "        emo2 = df[\"emotion\"][x+1]\n",
    "        ratio = fuzz.ratio(tweet1, tweet2)\n",
    "        if (ratio >= percent) & (emo1 == emo2):\n",
    "            dupes += 1\n",
    "            diff_emo += 1\n",
    "        elif ratio >= percent:\n",
    "            dupes += 1\n",
    "        else:\n",
    "            pass\n",
    "    print(str(percent) + \"%ers match \" + str(diff_emo/dupes) + \" of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So tweets that share almost nothing in common have the same label a little less than 60% of the time. I expect them to share the same label nearly 100% of the time as the two texts approach being identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:17.961085Z",
     "start_time": "2024-02-24T06:34:16.325524Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74%ers match 0.7668711656441718 of the time\n",
      "76%ers match 0.7673521850899743 of the time\n",
      "78%ers match 0.7671957671957672 of the time\n",
      "80%ers match 0.7677329624478443 of the time\n",
      "82%ers match 0.7652173913043478 of the time\n",
      "84%ers match 0.768160741885626 of the time\n",
      "86%ers match 0.7710437710437711 of the time\n",
      "88%ers match 0.7681159420289855 of the time\n",
      "90%ers match 0.7766599597585513 of the time\n",
      "92%ers match 0.7801932367149759 of the time\n",
      "94%ers match 0.7689969604863222 of the time\n",
      "96%ers match 0.773109243697479 of the time\n",
      "98%ers match 0.7769784172661871 of the time\n",
      "100%ers match 0.8214285714285714 of the time\n"
     ]
    }
   ],
   "source": [
    "#High similarity\n",
    "for percent in range(74,101,2):\n",
    "    diff_emo = 0\n",
    "    dupes = 0\n",
    "    for x in range(len(df)-2):\n",
    "        tweet1 = df[\"processed_text\"][x]\n",
    "        tweet2  = df[\"processed_text\"][x+1]\n",
    "        emo1 = df[\"emotion\"][x]\n",
    "        emo2 = df[\"emotion\"][x+1]\n",
    "        ratio = fuzz.ratio(tweet1, tweet2)\n",
    "        if (ratio >= percent) & (emo1 == emo2):\n",
    "            dupes += 1\n",
    "            diff_emo += 1\n",
    "        elif ratio >= percent:\n",
    "            dupes += 1\n",
    "        else:\n",
    "            pass\n",
    "    print(str(percent) + \"%ers match \" + str(diff_emo/dupes) + \" of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or... not? Even identical tweets are only matching 81% of the time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:18.086010Z",
     "start_time": "2024-02-24T06:34:17.961976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at sxsw apple schools the marketing experts link\n",
      "at sxsw apple schools the marketing experts link\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "does your smallbiz need reviews to play on google places we got an app for that link seo sxsw\n",
      "does your smallbiz need reviews to play on google places we got an app for that link seo sxsw\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "at sxsw apple schools the marketing experts link\n",
      "at sxsw apple schools the marketing experts link\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "mention google will connect the digital amp physical worlds through mobile link sxsw rt mention\n",
      "mention google will connect the digital amp physical worlds through mobile link sxsw rt mention\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "win free ipad from webdoc com sxsw rt\n",
      "win free ipad from webdoc com sxsw rt\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention what is going on at sxsw today share photos video with ireport link or through cnn iphone app\n",
      "rt mention what is going on at sxsw today share photos video with ireport link or through cnn iphone app\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "rt mention marissa mayer google will connect the digital amp physical worlds through mobile link sxsw\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention rt mention it is not rumor apple is opening up temporary store in downtown austin for sxsw and the ipad2 launch link\n",
      "rt mention rt mention it is not rumor apple is opening up temporary store in downtown austin for sxsw and the ipad launch link\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "rt mention rt mention it is not rumor apple is opening up temporary store in downtown austin for sxsw and the ipad launch link\n",
      "rt mention rt mention it is not rumor apple is opening up temporary store in downtown austin for sxsw and the ipad launch link\n",
      "Positive\n",
      "Not positive\n",
      "100\n",
      "\n",
      "\n",
      "score free am going to tshirt outside the sxsw apple store today at 15 pm amp check out am going to app for the ipad link sxsw ipad2\n",
      "score free am going to tshirt outside the sxsw apple store today at 15 pm amp check out am going to app for the ipad link sxsw ipad2\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n",
      "apple set to open popup shop in core of sxsw action in downtown austin link ipad\n",
      "apple set to open popup shop in core of sxsw action in downtown austin link ipad\n",
      "Not positive\n",
      "Positive\n",
      "100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(df)-2):\n",
    "    tweet1 = df[\"processed_text\"][x]\n",
    "    tweet2  = df[\"processed_text\"][x+1]\n",
    "    emo1 = df[\"emotion\"][x]\n",
    "    emo2 = df[\"emotion\"][x+1]\n",
    "    ratio = fuzz.ratio(tweet1, tweet2)\n",
    "    if (ratio == 100) & (emo1 != emo2):\n",
    "        print(tweet1)\n",
    "        print(tweet2)\n",
    "        print(emo1)\n",
    "        print(emo2)\n",
    "        print(ratio)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first reaction to these identical tweets getting two different scores was to panic. But, upon further review, I think that I can say that these are tweets with ambiguous emotional statements. Two different people (or the same person twice) rated them differently, and just reading them over I can see how they are debatable.\n",
    "\n",
    "To untangle this, I'm going to have to find some semi-arbitrary cutoff where the difference in categorization is likely to be because of the difference in text as opposed to a difference in interpretation. Anything more similar than that cutoff will be taken out of the population, meaning my NLP algorithm will be dealing with less ambiguous data. Ultimately, this is a good thing to have discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:18.554343Z",
     "start_time": "2024-02-24T06:34:18.086920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "80\n",
      "40 of google maps usage is mobile now has 150 million mobile users sxsw via mention\n",
      "40 of google maps usage is mobile wow mention sxsw via mention\n",
      "\n",
      "\n",
      "rt mention quot google before you tweet is the new think before you speak quot belinsky of digital democracy sxsw 911tweets\n",
      "rt mention quot google before you tweet quot is the new quot think before you speak quot mark belinsky sxsw\n",
      "\n",
      "\n",
      "rt mention rt mention google set to launch new social network circles today at sxsw\n",
      "rt mention rt mention google to launch social network circles at sxsw link who is excited\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "82\n",
      "ûï mention apple store downtown austin open til midnight sxsw mention there is still time\n",
      "ûï mention apple store downtown austin open til midnight sxsw any ipad2 left in stock\n",
      "\n",
      "\n",
      "apple set to open popup shop in core of sxsw action link sxsw apple ipad retail\n",
      "apple set to open popup shop in core of sxsw action link sxsw apple apple set to open popup sh link\n",
      "\n",
      "\n",
      "rt mention apple popup store at sxsw link\n",
      "rt mention apple popup store at sxsw link sat am at opening\n",
      "\n",
      "\n",
      "rt mention apple to open temporary store for sxsw at 6th and congress link atx\n",
      "rt mention apple to open temporary store for sxsw interesting link\n",
      "\n",
      "\n",
      "rt mention at sxsw apple schools the marketing experts link sxsw\n",
      "rt mention at sxsw apple schools the marketing experts link via mention sxsw apple marketing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "84\n",
      "google to launch major new social network called circles possibly today link sxsw gt gt should be good\n",
      "google to launch major new social network called circles possibly today link sxsw of course another social network\n",
      "\n",
      "\n",
      "apple set to open popup shop in core of sxsw action link ipad2 on the ground\n",
      "apple set to open popup shop in core of sxsw action link geeks need ipad2 love\n",
      "\n",
      "\n",
      "at sxsw apple schools the marketing experts link apple jobsco sxsw\n",
      "at sxsw apple schools the marketing experts link sxsw essdub\n",
      "\n",
      "\n",
      "the next big thing hmmm rt mention google to launch major new social network called circles possibly today link sxsw\n",
      "could be big gt gt google to launch major new social network called circles possibly today link tech sxsw\n",
      "\n",
      "\n",
      "40 of google maps use is mobile marissamayer sxsw\n",
      "40 of google maps use is mobile says mention sxsw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "86\n",
      "at sxsw apple schools the marketing experts sxsw cnet blogs link\n",
      "at sxsw apple schools the marketing experts link\n",
      "\n",
      "\n",
      "google to launch new social network at sxsw link google sxsw\n",
      "google to launch new social network at sxsw cnet news link sxsw\n",
      "\n",
      "\n",
      "the ipad design panel in hilton salon is filling up quickly no surprises there tapworthy sxsw\n",
      "the ipad design panel in hilton salon is filling up quickly no surprises there tapworthy sxsw it is more fun with the ipad2\n",
      "\n",
      "\n",
      "rt mention come party down with mention amp google tonight at sxsw link bands food art interactive maps\n",
      "rt mention come party mention and google tonight at sxsw link bands food art ice cream nifty interactive maps\n",
      "\n",
      "\n",
      "rt mention google to launch major new social network called circles possibly today link sxsw gt gt should be good\n",
      "rt mention google to launch major new social network called circles possibly today link rt mention sxsw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "88\n",
      "ûï mention google to launch major new social network called circles possibly today link sxsw super interesting\n",
      "ûï mention google to launch major new social network called circles possibly today link rt mention sxsw\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw pakistan cwc2011\n",
      "google to launch major new social network called circles possibly today link sxsw lb via mention\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw mention\n",
      "google to launch major new social network called circles possibly today link sxsw gt gt should be good\n",
      "\n",
      "\n",
      "google to launch new social network at sxsw cnet news link sxsw\n",
      "google to launch new social network at sxsw link aclu sxsw\n",
      "\n",
      "\n",
      "finally yeaayyy rt mention new ubersocial for iphone now in the app store includes uberguide to sxsw sponsored by mashable\n",
      "akhirnya ûï mention new ubersocial for iphone now in the app store includes uberguide to sxsw sponsored by link\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "90\n",
      "google to launch major new social network called circles possibly today updated at sxsw link\n",
      "google to launch major new social network called circles possibly today updated by mention link sxsw\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link\n",
      "google to launch major new social network called circles possibly today link via mention sxsw\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw mention\n",
      "google to launch major new social network called circles possibly today link sxsw pakistan cwc2011\n",
      "\n",
      "\n",
      "check out the free mention sampler on itunes link sxsw\n",
      "check out the free sxsw sampler on itunes link sxsw\n",
      "\n",
      "\n",
      "marissa mayer google will connect the digital amp physical worlds through mobile link mobile sxsw\n",
      "marissa mayer google will connect the digital amp physical worlds through mobile link google sxsw mobile hotpot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "92\n",
      "ûï mention google to launch major new social network called circles possibly today link sxsw mention\n",
      "ûï mention google to launch major new social network called circles possibly today link sxsw mention bigger than gaga\n",
      "\n",
      "\n",
      "google to launch major new social network called circles link sxsw via mention\n",
      "google to launch major new social network called circles link sxsw\n",
      "\n",
      "\n",
      "check out this video to get glimpse at what the action was like the ipad ipad sxsw gadgets link\n",
      "check out this video to get glimpse at what the action was like the ipad ipad sxsw gadgets link could not resist\n",
      "\n",
      "\n",
      "apple set to open popup shop in core of sxsw action link sxsw apple\n",
      "apple set to open popup shop in core of sxsw action link sxsw apple ipad retail\n",
      "\n",
      "\n",
      "rt mention quot google before you tweet quot is the new quot think before you speak quot mark belinsky sxsw\n",
      "rt mention quot google before you tweet quot is the new quot think before you speak quot mark belinsky 911tweets panel at sxsw\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "94\n",
      "kawasaki quot pagemaker saved apple quot oh those were the days sxsw jwtatl enchantment\n",
      "kawasaki quot pagemaker saved apple quot oh those were the days sxsw jwtatl enchantment via mention\n",
      "\n",
      "\n",
      "new post iphone apps we will be using at south by southwest interactive link sxsw sxswi\n",
      "new post iphone apps we will be using at south by southwest interactive sxsw sxswi link\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw lb via mention\n",
      "google to launch major new social network called circles possibly today link sxsw rt mention via mention\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw rt mention via mention\n",
      "google to launch major new social network called circles possibly today link sxsw via mention\n",
      "\n",
      "\n",
      "google to launch major new social network called circles possibly today link sxsw\n",
      "google to launch major new social network called circles link possibly today sxsw\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for percent in range(80,96,2):\n",
    "    counter = 0\n",
    "    print(\"\\n\")\n",
    "    print(percent)\n",
    "    for x in range(len(df)-2):\n",
    "        if counter == 5: #Looking at seven pairs of tweets per %\n",
    "            counter = 0\n",
    "            break\n",
    "        else:\n",
    "            tweet1 = df[\"processed_text\"][x]\n",
    "            tweet2  = df[\"processed_text\"][x+1]\n",
    "            emo1 = df[\"emotion\"][x]\n",
    "            emo2 = df[\"emotion\"][x+1]\n",
    "            ratio = fuzz.ratio(tweet1, tweet2)\n",
    "            if (ratio == percent) & (emo1 != emo2):\n",
    "                print(tweet1)\n",
    "                print(tweet2)\n",
    "                print(\"\\n\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like new useful information peters out in the high 80s with raw text. But does that remain true if we remove all the stopwords and lemmatize the text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll make a new column out of the processed_text where just the most popular (and hopefully useful) word pieces will be left behind.\n",
    "\n",
    "The first step is just setting up the regex filter and creating an empty column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:51:06.593287Z",
     "start_time": "2024-02-24T06:51:05.876963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reusing the punctuation and numeral regex filter from earlier in the course\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "df[\"tokens\"] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df[\"tokens\"][i] = nltk.regexp_tokenize(df[\"processed_text\"][i], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, removing stopwords. I already jumped ahead and saw which words show up most often in the FreqDist, so I added those to the stopwords list at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:19.165891Z",
     "start_time": "2024-02-24T06:34:19.162785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [0, 1, 2, 3]\n",
    "range(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:19.973868Z",
     "start_time": "2024-02-24T06:34:19.167566Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += [str(i) for i in range(10)]\n",
    "stopwords_list += [\"sxsw\", \"link\", \"mention\"] #The top words by far\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df[\"tokens\"][i] = [w for w in df[\"tokens\"][i] if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to reduce each word to its lemma, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:19.977268Z",
     "start_time": "2024-02-24T06:34:19.975390Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:20.790067Z",
     "start_time": "2024-02-24T06:34:19.978867Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)-1):\n",
    "    new_sentence = []\n",
    "    for word in df[\"tokens\"][i]:\n",
    "        new_sentence.append(lemmatizer.lemmatize(word))\n",
    "    df[\"tokens\"][i] = new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:20.859452Z",
     "start_time": "2024-02-24T06:34:20.791049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 2961),\n",
       " ('rt', 2935),\n",
       " ('google', 2616),\n",
       " ('apple', 2309),\n",
       " ('quot', 1657),\n",
       " ('iphone', 1559)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for i in range(len(df)-1):\n",
    "    for w in df[\"tokens\"][i]:\n",
    "        all_tokens.append(w)\n",
    "\n",
    "freq = FreqDist(all_tokens)\n",
    "freq.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to leave \"quot\" and \"rt\" in case people link to things, quote people, or retweet more often based on a particular emotional reaction to those things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:22.793220Z",
     "start_time": "2024-02-24T06:34:20.860326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85%ers match 0.7779751332149201 of the time\n",
      "86%ers match 0.7807971014492754 of the time\n",
      "87%ers match 0.7844990548204159 of the time\n",
      "88%ers match 0.7883495145631068 of the time\n",
      "89%ers match 0.7814432989690722 of the time\n",
      "90%ers match 0.7871396895787139 of the time\n",
      "91%ers match 0.789838337182448 of the time\n",
      "92%ers match 0.7878787878787878 of the time\n",
      "93%ers match 0.7890173410404624 of the time\n",
      "94%ers match 0.7917981072555205 of the time\n",
      "95%ers match 0.7842465753424658 of the time\n",
      "96%ers match 0.7887931034482759 of the time\n",
      "97%ers match 0.791907514450867 of the time\n",
      "98%ers match 0.8035714285714286 of the time\n",
      "99%ers match 0.8035714285714286 of the time\n",
      "100%ers match 0.8035714285714286 of the time\n"
     ]
    }
   ],
   "source": [
    "for percent in range(85,101):\n",
    "    diff_emo = 0\n",
    "    dupes = 0\n",
    "    for x in range(len(df)-2):\n",
    "        tweet1 = df[\"tokens\"][x]\n",
    "        tweet2  = df[\"tokens\"][x+1]\n",
    "        emo1 = df[\"emotion\"][x]\n",
    "        emo2 = df[\"emotion\"][x+1]\n",
    "        ratio = fuzz.ratio(tweet1, tweet2)\n",
    "        if (ratio >= percent) & (emo1 == emo2):\n",
    "            dupes += 1\n",
    "            diff_emo += 1\n",
    "        elif ratio >= percent:\n",
    "            dupes += 1\n",
    "        else:\n",
    "            pass\n",
    "    print(str(percent) + \"%ers match \" + str(diff_emo/dupes) + \" of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this seems to be about the same results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:34:22.926163Z",
     "start_time": "2024-02-24T06:34:22.794152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['line', 'apple', 'store', 'ipad']\n",
      "['line', 'apple', 'store', 'ipad', 'day']\n",
      "Not positive\n",
      "Positive\n",
      "89\n",
      "\n",
      "\n",
      "['apple', 'set', 'open', 'popup', 'shop', 'core', 'action', 'apple']\n",
      "['apple', 'set', 'open', 'popup', 'shop', 'core', 'action', 'apple', 'ipad', 'retail']\n",
      "Not positive\n",
      "Positive\n",
      "89\n",
      "\n",
      "\n",
      "['google', 'map', 'usage', 'mobile', 'yowza']\n",
      "['google', 'map', 'usage', 'mobile']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n",
      "['google', 'map', 'use', 'mobile']\n",
      "['google', 'map', 'use', 'mobile', 'marissamayer']\n",
      "Not positive\n",
      "Positive\n",
      "89\n",
      "\n",
      "\n",
      "['rt', 'presentation', 'demonstrates', 'pause', 'video', 'new', 'video', 'player', 'pick', 'spot', 'ipad', 'app', 'tveverywhere']\n",
      "['rt', 'preso', 'demonstrates', 'pause', 'video', 'new', 'video', 'player', 'amp', 'pick', 'spot', 'ipad', 'app', 'tveverywhere']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n",
      "['rt', 'new', 'ubersocial', 'iphone', 'app', 'store', 'includes', 'uberguide', 'got']\n",
      "['rt', 'new', 'ubersocial', 'iphone', 'app', 'store', 'includes', 'uberguide', 'sponsored']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n",
      "['rt', 'rt', 'google', 'launch', 'major', 'new', 'social', 'network', 'called', 'circle', 'possibly', 'today', 'cool']\n",
      "['rt', 'rt', 'google', 'launch', 'major', 'new', 'social', 'network', 'called', 'circle', 'possibly', 'today', 'via', 'pcbuzz']\n",
      "Not positive\n",
      "Positive\n",
      "89\n",
      "\n",
      "\n",
      "['rt', 'rt', 'google', 'launch', 'major', 'new', 'social', 'network', 'called', 'circle', 'possibly', 'today', 'via', 'pcbuzz']\n",
      "['rt', 'rt', 'google', 'launch', 'major', 'new', 'social', 'network', 'called', 'circle', 'possibly', 'today', 'sxswi']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n",
      "['rt', 'ipad', 'take', 'video', 'sxswi']\n",
      "['rt', 'ipad', 'take', 'video']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n",
      "['going', 'today', 'share', 'photo', 'video', 'ireport', 'cnn', 'iphone', 'app']\n",
      "['friend', 'going', 'today', 'share', 'photo', 'video', 'cnn', 'iphone', 'app']\n",
      "Positive\n",
      "Not positive\n",
      "89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's all the adjacent 89s for reference.\n",
    "for x in range(len(df)-2):\n",
    "    tweet1 = df[\"tokens\"][x]\n",
    "    tweet2  = df[\"tokens\"][x+1]\n",
    "    emo1 = df[\"emotion\"][x]\n",
    "    emo2 = df[\"emotion\"][x+1]\n",
    "    ratio = fuzz.ratio(tweet1, tweet2)\n",
    "    if (ratio ==89) & (emo1 != emo2):\n",
    "        print(tweet1)\n",
    "        print(tweet2)\n",
    "        print(emo1)\n",
    "        print(emo2)\n",
    "        print(ratio)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that a lot of the difference is in just a word or two. Sometimes it makes sense (adding \"good\" to a message) and other times it's just a transposition of letters or adding a name. So at this % range there's a fair mix of reasons why messages are different. To be fair, I'll set the bar at 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "#### I am going to run an experiment: I'll train one model with the most \"ambiguous\" texts removed, and train one model without removing anything. My prediction is that the model that is trained without those edge cases will have an easier time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Identify the similar tweets and set their IDs aside.\n",
    "\n",
    "Step 2: Find out how many words show up more than once. If there are at least a few thousand, then those I'll remove words that show up only once.\n",
    "\n",
    "Step 3: Create a duplicate DF and remove the ambiguous tweets from one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:59:34.213443Z",
     "start_time": "2024-02-24T06:53:15.024733Z"
    }
   },
   "outputs": [],
   "source": [
    "# isolating ones with different emotions but high similarity\n",
    "ambig_emo = []\n",
    "for x in range(len(df)-2):\n",
    "        tweet1 = df[\"tokens\"][x]\n",
    "        for y in range(len(df)-1):\n",
    "            tweet2 = df[\"tokens\"][y]\n",
    "            ratio = fuzz.ratio(tweet1, tweet2)\n",
    "            if ratio < 89:\n",
    "                pass\n",
    "            elif df[\"emotion\"][x] == df[\"emotion\"][y]:\n",
    "                pass\n",
    "            elif x==y:\n",
    "                pass\n",
    "            else:\n",
    "                ambig_emo.append(x)\n",
    "                ambig_emo.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:59:37.400055Z",
     "start_time": "2024-02-24T06:59:37.396889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357\n",
      "0.15187465025181868\n"
     ]
    }
   ],
   "source": [
    "print(len(set(ambig_emo))) #1329\n",
    "print(len(set(ambig_emo))/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:40:01.686471Z",
     "start_time": "2024-02-24T06:40:01.682521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4816\n"
     ]
    }
   ],
   "source": [
    "single_words = 0\n",
    "for i in list(freq.values()):\n",
    "    if i == 1:\n",
    "        single_words += 1\n",
    "\n",
    "#The entire frequency distribution minus the words which only appear once.\n",
    "print(len(freq)-single_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:40:01.701532Z",
     "start_time": "2024-02-24T06:40:01.687312Z"
    }
   },
   "outputs": [],
   "source": [
    "common_words = []\n",
    "for item in freq.most_common(4806):\n",
    "    common_words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:40:03.361769Z",
     "start_time": "2024-02-24T06:40:01.702519Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df[\"tokens\"][i] = [w for w in df[\"tokens\"][i] if w in common_words]\n",
    "    df[\"tokens\"][i] = \" \".join(df[\"tokens\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:59:40.551425Z",
     "start_time": "2024-02-24T06:59:40.546969Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"sdf\" is the alternate dataframe with edge cases removed. s=small/secondary\n",
    "sdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T06:59:43.765082Z",
     "start_time": "2024-02-24T06:59:43.759214Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf.drop(sdf.index[ambig_emo], inplace = True)\n",
    "sdf.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T07:18:30.338839Z",
     "start_time": "2024-02-24T07:18:30.245297Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tokens = []\n",
    "for i in range(len(df)):\n",
    "    df_tokens.append(\" \".join(df[\"tokens\"][i]))\n",
    "    \n",
    "sdf_tokens = []\n",
    "for i in range(len(sdf)):\n",
    "    sdf_tokens.append(\" \".join(sdf[\"tokens\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T06:42:43.835253Z",
     "start_time": "2024-02-26T06:42:43.798858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Features to build the model\n",
    "X = df_tokens\n",
    "y = df[['emotion']]\n",
    "\n",
    "X_s = sdf_tokens\n",
    "y_s = sdf[[\"emotion\"]]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=42)\n",
    "## SDF\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_s, y_s,\n",
    "                                                        random_state=4)\n",
    "\n",
    "# And a secondary train-test split for validating the models\n",
    "\n",
    "\n",
    "#\n",
    "X_tr2, X_te2, y_tr2, y_te2 = train_test_split(X_train, y_train, \n",
    "                                              random_state=42)\n",
    "## SDF\n",
    "Xs_tr2, Xs_te2, ys_tr2, ys_te2 = train_test_split(Xs_train, ys_train,\n",
    "                                                        random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:40:04.426377Z",
     "start_time": "2024-02-26T07:40:04.150124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit-transforming onto the validation set's training data\n",
    "cv = CountVectorizer(max_features=4816)\n",
    "cvs = CountVectorizer(max_features=4816)\n",
    "\n",
    "X_tr2_vec = cv.fit_transform(X_tr2)\n",
    "X_tr2_vec = pd.DataFrame.sparse.from_spmatrix(X_tr2_vec)\n",
    "X_tr2_vec.columns = sorted(cv.vocabulary_)\n",
    "X_tr2_vec.set_index(y_tr2.index, inplace = True)\n",
    "\n",
    "#s\n",
    "Xs_tr2_vec = cvs.fit_transform(Xs_tr2)\n",
    "Xs_tr2_vec = pd.DataFrame.sparse.from_spmatrix(Xs_tr2_vec)\n",
    "Xs_tr2_vec.columns = sorted(cvs.vocabulary_)\n",
    "Xs_tr2_vec.set_index(ys_tr2.index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T06:43:08.466874Z",
     "start_time": "2024-02-26T06:43:08.353663Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_te2_vec = cv.transform(X_te2)\n",
    "X_te2_vec = pd.DataFrame.sparse.from_spmatrix(X_te2_vec)\n",
    "X_te2_vec.columns = sorted(cv.vocabulary_)\n",
    "X_te2_vec.set_index(y_te2.index, inplace=True)\n",
    "\n",
    "#s\n",
    "Xs_te2_vec = cv.transform(Xs_te2)\n",
    "Xs_te2_vec = pd.DataFrame.sparse.from_spmatrix(Xs_te2_vec)\n",
    "Xs_te2_vec.columns = sorted(cv.vocabulary_)\n",
    "Xs_te2_vec.set_index(ys_te2.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T21:59:59.433104Z",
     "start_time": "2024-02-24T21:59:59.219633Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_tr2_vec, y_tr2[\"emotion\"].ravel())\n",
    "\n",
    "mnb_s = MultinomialNB()\n",
    "mnb_s.fit(Xs_tr2_vec, ys_tr2[\"emotion\"].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T06:34:59.455914Z",
     "start_time": "2024-02-26T06:34:59.341100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7267303102625299\n"
     ]
    }
   ],
   "source": [
    "y_hat = mnb.predict(X_te2_vec)\n",
    "print(\"accuracy = \" + str(accuracy_score(y_te2, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:53:26.196671Z",
     "start_time": "2024-02-25T00:53:26.164654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[926, 197],\n",
       "       [261, 292]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_te2, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T22:07:11.858253Z",
     "start_time": "2024-02-24T22:07:11.734798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219563687543983"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_hat = mnb.predict(Xs_te2_vec)\n",
    "accuracy_score(ys_te2, ys_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T00:53:53.118193Z",
     "start_time": "2024-02-25T00:53:53.096639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[858, 115],\n",
       "       [138, 310]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix.(ys_te2, ys_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:08:52.361008Z",
     "start_time": "2024-02-26T09:04:42.901358Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "\"n_estimators\": [100, 500, 1000, 2000, 4000],\n",
    "\"max_depth\": [2, 4, 6],\n",
    "\"min_samples_split\": [2, 5, 10],\n",
    "\"min_samples_leaf\": [1, 3, 5]}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rfs = RandomForestClassifier()\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=param_grid, cv = 3, return_train_score=True).fit(X_tr2_vec, y_tr2[\"emotion\"].ravel())\n",
    "\n",
    "rfs_grid = GridSearchCV(rfs, param_grid=param_grid, cv = 3, return_train_score=True).fit(Xs_tr2_vec, ys_tr2[\"emotion\"].ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:30:17.902195Z",
     "start_time": "2024-02-26T09:30:17.896238Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df best features = {'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "original sdf best features = {'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"original df best features = \" + str(rf_grid.best_params_))\n",
    "print(\"original sdf best features = \" + str(rfs_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:41:18.991564Z",
     "start_time": "2024-02-26T09:41:18.985598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "{'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "print(rf_grid2.best_params_)\n",
    "print(rfs_grid2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:53:43.045076Z",
     "start_time": "2024-02-26T09:53:40.140440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df accuracy score = 0.6742243436754176\n",
      "sdf accuracy score = 0.6847290640394089\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth= 6, \n",
    "                            min_samples_leaf= 3, min_samples_split= 5)\n",
    "\n",
    "rf.fit(X_tr2_vec, y_tr2[\"emotion\"].ravel())\n",
    "y_hat = rf.predict(X_te2_vec)\n",
    "print(\"original df accuracy score = \" + str(accuracy_score(y_hat, y_te2)))\n",
    "\n",
    "\n",
    "#s\n",
    "rfs = RandomForestClassifier(n_estimators=2000, max_depth= 6, \n",
    "                             min_samples_leaf= 3, min_samples_split= 5)\n",
    "\n",
    "rfs.fit(Xs_tr2_vec, ys_tr2[\"emotion\"].ravel())\n",
    "ys_hat = rfs.predict(Xs_te2_vec)\n",
    "print(\"sdf accuracy score = \" + str(accuracy_score(ys_hat, ys_te2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this evidence in favor of the smaller dataframe, I should iterate through some other possible boundaries over which tweets may have an ambiguous emotional categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T10:25:55.003876Z",
     "start_time": "2024-02-26T09:59:07.347589Z"
    }
   },
   "outputs": [],
   "source": [
    "ambig_list = []\n",
    "\n",
    "for i in range(68, 90, 7):\n",
    "    ambiguous_tweets = []\n",
    "    for x in range(len(df)-2):\n",
    "        tweet1 = df[\"tokens\"][x]\n",
    "        for y in range(len(df)-1):\n",
    "            tweet2 = df[\"tokens\"][y]\n",
    "            ratio = fuzz.ratio(tweet1, tweet2)\n",
    "            if ratio < i:\n",
    "                pass\n",
    "            elif df[\"emotion\"][x] == df[\"emotion\"][y]:\n",
    "                pass\n",
    "            elif x==y:\n",
    "                pass\n",
    "            else:\n",
    "                ambiguous_tweets.append(x)\n",
    "                ambiguous_tweets.append(y)\n",
    "    ambig_list.append(ambiguous_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T11:03:12.613908Z",
     "start_time": "2024-02-26T11:03:09.918981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for 0 = 0.7633228840125392\n",
      "acc for 1 = 0.745814307458143\n",
      "acc for 2 = 0.7685525349008082\n",
      "acc for 3 = 0.7494722026741731\n"
     ]
    }
   ],
   "source": [
    "for num, list_t in enumerate(ambig_list):\n",
    "    # Features to build the model\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    temp_df.drop(temp_df.index[list_t], inplace = True)\n",
    "    temp_df.reset_index(drop = True, inplace=True)\n",
    "    \n",
    "    temp_tokens = []\n",
    "    for n in range(len(temp_df)):\n",
    "        temp_tokens.append(\" \".join(temp_df[\"tokens\"][n]))\n",
    "    \n",
    "    X = temp_tokens\n",
    "    y = temp_df[\"emotion\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "    # The secondary train-test split for validating the models\n",
    "\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train,\n",
    "                                                            random_state=4)\n",
    "    # Fit-transforming onto the validation set's training data\n",
    "    cv = CountVectorizer(max_features=4816, lowercase=False)\n",
    "\n",
    "    X_train2_vec = cv.fit_transform(X_train2)\n",
    "    X_train2_vec = pd.DataFrame.sparse.from_spmatrix(X_train2_vec)\n",
    "    X_train2_vec.columns = sorted(cv.vocabulary_)\n",
    "    X_train2_vec.set_index(y_train2.index, inplace = True)\n",
    "\n",
    "    X_test2_vec = cv.transform(X_test2)\n",
    "    X_test2_vec = pd.DataFrame.sparse.from_spmatrix(X_test2_vec)\n",
    "    X_test2_vec.columns = sorted(cv.vocabulary_)\n",
    "    X_test2_vec.set_index(y_test2.index, inplace=True)\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train2_vec, y_train2)\n",
    "    \n",
    "    y_hat = mnb.predict(X_test2_vec)\n",
    "    print(\"acc for \" + str(num) + \" = \" + str(accuracy_score(y_test2, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd07168de47e55f922642e821276e82e1ba59be8ba89f9afd7a9ad5fcf10172704f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
